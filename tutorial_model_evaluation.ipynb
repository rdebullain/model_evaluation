{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f00e1d2",
   "metadata": {},
   "source": [
    "# Model Evaluation - Tutorial\n",
    "\n",
    "In the last couple of readings, we learned a lot about various model evaluation methods. Now, it's time to get our hands dirty by practicing the evaluation techniques with the `Sklearn` library. \n",
    "\n",
    "## Regression Models Evaluation\n",
    "\n",
    "To demonstrate evaluation, we won't be using any regression model but will generate original values and predictions from the model by `Numpy` instead. Let's begin by importing `Numpy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba148ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a390bd3",
   "metadata": {},
   "source": [
    "We will generate 10 observations (`y_true`) and 10 predictions (`y_pred`) from a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d367f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'ground truth'\n",
    "y_true = np.random.normal(0,1,10)\n",
    "\n",
    "# generate random errors\n",
    "errors = np.random.normal(0,0.02,10)\n",
    "\n",
    "# simulate predictions\n",
    "y_pred = y_true + errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1960b",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) / Root Means Squared Error (RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465c1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007448565616955083\n"
     ]
    }
   ],
   "source": [
    "# import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# compute MSE\n",
    "MSE = mean_squared_error(y_true,y_pred)  \n",
    "\n",
    "# print MSE\n",
    "print(MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec6f2d1",
   "metadata": {},
   "source": [
    "All regression evaluation functions from `sklearn.metrics` take two mandatory arrays as parameters. The first is an array with ground truth values (in our case `y_true` variable) and the second is our prediction (in our case `y_pred` variable).\n",
    "\n",
    "To get RMSE from MSE we have two options: the first option is to compute the square root from MSE by `Numpy` and the second option is to use the `squared=False` option in a function. Let's try both options:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbf93b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02729206041499081\n",
      "0.02729206041499081\n"
     ]
    }
   ],
   "source": [
    "# RMSE by Numpy\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# RMSE by sklearn\n",
    "RMSE = mean_squared_error(y_true,y_pred,squared=False)\n",
    "print(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1632f46",
   "metadata": {},
   "source": [
    "## Classification Models Evaluation\n",
    "\n",
    "We will use the same principle as in regression model evaluation and use synthetic data. With the only difference that we will need predicted probabilities instead of predicted labels (predicted values in regression). The important thing to mention is that we are simulating the behavior of a binary classifier. It means that the predicted class is only positive (returns 1) or negative (returns 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af353f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "y_true = [1,1,0,1,0,0,1,0,0,1]\n",
    "\n",
    "# simulate probabilites of positive class\n",
    "y_proba = [0.9,0.7,0.2,0.99,0.7,0.1,0.5,0.2,0.4,0.6]\n",
    "\n",
    "# set the threshold to predict positive class\n",
    "thres = 0.5\n",
    "\n",
    "# class predictions\n",
    "y_pred = [int(value > thres) for value in y_proba]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42febf8",
   "metadata": {},
   "source": [
    "In the variable `y_proba`, we have the probabilities of the observations from the positive class. We also set the `threshold` value. All observations with probabilities above this threshold are assigned to the positive class and stored in the `y_pred` variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e30a8",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c17af46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# import accuracy_score from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "\n",
    "# print accuracy\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d25829",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bcef6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "# import f1_score from sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# compute F1-score\n",
    "f1_score = f1_score(y_true,y_pred)\n",
    "\n",
    "# print F1-score\n",
    "print(f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33a697",
   "metadata": {},
   "source": [
    "### AUC-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f599d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# import roc_auc_score from sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# compute AUC-score\n",
    "auc = roc_auc_score(y_true,y_proba)\n",
    "\n",
    "# print AUC-score\n",
    "print(auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26995358",
   "metadata": {},
   "source": [
    "> #### Note\n",
    "> In `roc_auc_score` we use probabilities (`y_proba`) instead of class labels. If we passed class labels no errors would be shown but the score would be inaccurate. Be careful and **read the documentation** before using  `Sklearn` functions.\n",
    "\n",
    "As we can see in the examples above, evaluation with `Sklearn` is pretty straighforward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83d5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
